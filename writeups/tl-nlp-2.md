# Transfer Learning in NLP (part II)

The third paper I have read about for transfer learning in NLP is [Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf) D.Cera, Y.Yanga, S.Konga, N.Huaa, N.Limtiacob, R.St. Johna, N.Constanta, M.Guajardo-Ce ÃÅspedesa, S.Yuanc, C.Tara, Y.Sunga, B.Stropea, R.Kurzweila

* As first two papers about this topic, the paper tries to train a universal or general sentence encoder which can encode a sentence into a fix dimentional vector. 
* This paper uses the same scheme for this task, which is to find encoders, train it with several tasks to capture the general representation, then evaluate the effectiveness and robustness of transferred sentence representation using several transfer learning classification tasks.
* This paper explores two encoders, [Transformer](https://arxiv.org/pdf/1706.03762.pdf) and [DAN](https://www.aclweb.org/anthology/P15-1162.pdf). There is a trade-off between accuracy and memory-training time. Transformer is complex and taking long time to train but can achieve higher accuracy, while DAN is simple a model but with a little bit lower performance. 
* Both word and sentence transfer are investigated. Word Embedding transfer means that only using general word vectors but sentence encoders are trained using the task domain, while sentence embedding transfer means that sentence encoder is fixed when applied for transfer tasks.  It turns out that sentence transfer is more effective and robust than just word embedding transfer and dual transfer will improve further. I guess the reason that the sentence transfer will get better since itself already includes some information of more general word embedding. 

Through those papers, it seems that **transfer learning** and **multitask learning** are the trend for NLP field to find a encoder which can extract information as general as possible.  I think next problems are how to (1) find effective encoder architectures (2) find training tasks and methods to capture different inducive biases (3) balance the trade-off between accuracy and complexity. (4) distill knowledge from complex models and use the simple model to acchieve as much as possible.  